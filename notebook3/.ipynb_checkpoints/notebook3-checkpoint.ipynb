{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pill 3: About data\n",
    "\n",
    "## 1. Exploring some data\n",
    "\n",
    "In order to motivate this topic let us first explore a couple of datasets. The first one comes from Eurostat, I have downloaded for you data from Gross domestic expenditure on R&D (GERD) by source of funds (tsc00031), but you can take any other file. Let us look at the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\" ><b>QUESTION:</b> What do we find in this data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check another example. This time this is a file from a telecom, and our goal is to predict *churn*. Let us have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load './files/churn_small.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\"><b>QUESTION:</b> What do we find in this data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill 3: About data\n",
    "\n",
    "What we have found so far:\n",
    "\n",
    "+ Heterogeneous data\n",
    "+ Missing data\n",
    "+ Categorical data\n",
    "+ Large dimensionality\n",
    "+ Outliers\n",
    "\n",
    "What to do with all these pathologies?\n",
    "\n",
    "\n",
    "\n",
    "Goals of this session:\n",
    "\n",
    "+ Pandas exists\n",
    "+ Feature extraction\n",
    "+ Dealing with categorical data\n",
    "    + One-hot encoding\n",
    "    + Hashing trick\n",
    "+ Dealing with missing data\n",
    "+ Dealing with outliers\n",
    "+ The curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is a Python package providing fast, flexible, and expressive data structures designed to work with relational or labeled data both. It is a fundamental high-level building block for doing practical, real world data analysis in Python.\n",
    "\n",
    "pandas is well suited for:\n",
    "\n",
    "+ Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "+ Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "+ Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames \n",
    "DataFrames are designed to store heterogeneous multivarite data, where for every index there are multiple fields or columns of data, often of different data type.\n",
    "\n",
    "A `DataFrame` os a tabular data structure, encapsulating multiple series like columns in a spreadsheet. Data are stored interally as a 2-dimensional object, but the `DataFrame` allows us to represent and manipulate higher-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading tabular data\n",
    "\n",
    "The ‘pandas’ Python library provides several operators, <code>read_csv(), read_table(), \n",
    "read_excel() ...</code> that allows you to access data ﬁles in tabular format on your computer as well as data stored in web repositories.\n",
    "\n",
    "Reading in a data table is simply a matter of knowing the name (and location) of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./files/churn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the first or last rows of the data frame using `head` or  `tail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Let us describe a little bit what we are seeing.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in CSV and databases are often organized in what is called *stacked* or *record* formats. In this case for each record we have 21 different attributes or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns.tolist()\n",
    "\n",
    "print(\"Column names:\")\n",
    "print(col_names)\n",
    "\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "\n",
    "print (\"\\nSample data:\")\n",
    "df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the indexes can be retrieved using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can return the values as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the values of a column indexing by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Area Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we slice then we are retrieving rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can work the same way you do in numpy using `.ix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ix[0:3,['Area Code', 'Phone']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is useful for joining, slicing, selecting and cleaning data. There is much much more, but we will basically use numpy so, sufices to know the very basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Cleaning Eurostat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the Eurostat file is a mixed file, we can read it according to the specific TAB defined format and the work out the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./files/tsc00031.tsv', sep='\\t',encoding=\"utf-8-sig\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the cells using the commas, and create as many columns as elements according to the splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = 'sectfund,unit,geo\\\\time'.split(',')\n",
    "df[cols] = df['sectfund,unit,geo\\\\time'].str.split(',', expand=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can drop the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(labels=['sectfund,unit,geo\\\\time'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style=\"border-radius:10px;border-width:3px\">Let us try to clean a little bit this file.\n",
    "<ol>\n",
    "<li> Change all ':' for 'NaN'</li>\n",
    "<li> Remove the letters in the numerical valued columns</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = df.replace(': ',np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete everything that i not a number or a point from the numerical columns\n",
    "\n",
    "df[data_columns[:-3]]=df[data_columns[:-3]].replace(r'[^0-9\\.]','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change to NAN elements without information\n",
    "\n",
    "df[data_columns[:-3]]=df[data_columns[:-3]].replace(r'',np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[data_columns[:-3]]=df[data_columns[:-3]].astype(np.float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels = ['unit'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Handling Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different casuistics for dealing with missing data. The following diagram shows some examples of how to deal with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./files/missing_data.jpg' width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of replacing missing data with another value is called **inputing**.\n",
    "\n",
    "There are basicaly three ways of dealing with missing data:\n",
    "\n",
    "+ Deletion:\n",
    "\n",
    "    + Completely remove data sample. This can be dangerous if the data set size is small. \n",
    "    + Pair-wise deletion. If we are making different analysis each involving different subsets of attributes, then for each specific analysis we could just remove the samples affected by missing data for that specific analysis instead of doing that for the complete data set first and then proceed with the analysis. In this sense we keep all possible data. This, however, does not necessary allow to fairly compare different analysis since they may involve different amount of data.\n",
    "    \n",
    "+ Single substitution:\n",
    "\n",
    "    + Replace missing data with mean/mode/median. \n",
    "    + Create a *dummy variable*, a new variable, indicating the value is missing and inpute missing data to a single value such as mean/median.\n",
    "    + Regression inputation. Use regression on available data to inpute the value.\n",
    "    \n",
    "+ Model-based methods\n",
    "    + Multiple imputation: The idea is to sample the inputed value from a distribution (it can be the empirical distribution) and create a set of inputed datasets (each dataset is created by sampling the values independently). The analysis is done over all of them and results are aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df['sectfund'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\"><b>QUESTION:</b> What should we do with this data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the amount of `NaN`, both column-wise and row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "v=np.array(df.values[:,:-2],dtype=np.float)\n",
    "\n",
    "fig = plt.figure()\n",
    "sf1 = plt.subplot(1,2,1)\n",
    "year_nan = np.sum(np.where(np.isnan(v),1.,0.),axis=0)\n",
    "plt.bar(np.array(range(year_nan.shape[0])),year_nan)\n",
    "plt.subplot(1,2,2)\n",
    "concept_nan = np.sum(np.where(np.isnan(v),1.,0.),axis=1)\n",
    "plt.bar(np.array(range(concept_nan.shape[0])),concept_nan)\n",
    "\n",
    "fig.set_size_inches((12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\"><b>QUESTION:</b> What should we do with this data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we could work with this data set to understand EU country policies with respect to research. We could ask what are the countries that have a similar profile of expediture in research.\n",
    "\n",
    "In order to answer this question we could work just with data from one year. Let us take 2012, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['2012 ','sectfund','geo\\\\time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent each country according to the differnt kind of expeditures. To do this we will use pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = df.pivot(index='geo\\\\time',columns='sectfund',values = '2012 ')\n",
    "df_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style =\"border-radius:10px;border-width:3px\"><b>EXERCISE:</b> We want to know what countries have more similar policies to each other. In order to do this, follow these steps:\n",
    "\n",
    "<ul>\n",
    "<li>Inpute missing values.</li>\n",
    "<li>We will be using unsupervised learning techniques. In particular k-means. You can import `k-means` form the module `sklearn.cluster`. Supose that we want to find three different clusters. Train the clustering technique and report the countries more similar to the Spanish research policy.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Imputation\n",
    "df_inputed= ...........\n",
    "\n",
    "#Clustering\n",
    "clf = kmeans(...........)\n",
    "\n",
    "idx = np.argsort(clf).astype(np.int)\n",
    "\n",
    "countries = np.array(df_inputed.index.tolist())\n",
    "\n",
    "plt.plot(clf[idx],'ro')\n",
    "plt.xticks(idx, countries, rotation='vertical')\n",
    "plt.gcf().set_size_inches((12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A toy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following two dimensional problem with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f \n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0,3,1000)\n",
    "y = x + 0.5 + 0.5*np.random.normal(size=x.shape[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply a simple linear regressor (`linear_model`) from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf = clf.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "yhat=clf.predict(x[:,np.newaxis])\n",
    "\n",
    "plt.plot(x,y,'ro',alpha=0.25)\n",
    "plt.plot(x,yhat,'b-',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_missing_rand = y[:]\n",
    "y_missing_rand = np.where(np.random.rand(x.shape[0])>0.5,np.nan,y)\n",
    "print(y_missing_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style =\"border-radius:10px;border-width:3px\"><b>EXERCISE:</b> Inpute missing values using the mean of the data and plot the data set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf = clf.fit(x[:,np.newaxis],y_inputed[:,np.newaxis])\n",
    "yhat_missing=clf.predict(x[:,np.newaxis])\n",
    "plt.plot(x,y,'ro',alpha=0.25)\n",
    "plt.plot(x,yhat,'b-',linewidth=3)\n",
    "plt.plot(x,yhat_missing,'y-',linewidth=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\"><b>QUESTION:</b> What can we do to correct this issue?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some intuitions about the feature space and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build our intuition about the feature extraction process. Cosider a classic problem of handwritten digits recognition. Let us load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data set.\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the data just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data format.\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.min(X),np.max(X))\n",
    "\n",
    "X = X/np.max(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style =\"border-radius:10px;border-width:3px\"><b>QUESTION:</b> What do the values of the attributes mean in this data set?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[4].reshape((8,8)),cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\" style = \"border-radius:10px;border-width:3px\">\n",
    "**QUESTION:** Consider the following two problems:\n",
    "\n",
    "<ul>\n",
    "<li> We are asked to develop a product for analyzing the text in a website. The goal is to TAG the site according to its content. </li>\n",
    "<li> We are asked to develop a product similar to Shazzam(tm). This is, recognize the name of a song given a small sample of the music.</li>\n",
    "<p>\n",
    "Discuss and describe a posible feature vector for this problem with your partner.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">\n",
    "**EXERCISE/QUIZ:**\n",
    "Train a K-nearest neighbor classifier (module ``neighbors``, class ``KNeighborsClassifier``) with $K=10$ on the digits data and check the classification accuracy score on a test set. Use a train_test split of 50% of data for training and 50% for testing purposes and seed value ``random_state=42``. Report the classification accuracy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Adding expert knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">\n",
    "**EXERCISE:**\n",
    "Using the same settings as before try to enrich the training set. For exaple, we could use symmetry or amount of pixels. Train a K-nearest neighbor classifier (module ``neighbors``, class ``KNeighborsClassifier``) with $K=10$ on the digits data using the same settings as before using the training set enriched with new data and check the classification accuracy score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature extraction: Adding interaction variables\n",
    "\n",
    "Interaction variables try to make explicit the correlation of the attributes. In order to do so, we can just consider the product of pairs, triples, ... of atributes. Consider the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reset -f\n",
    "import numpy as np\n",
    "x_test = np.linspace(0,10,50)\n",
    "x = 10*np.random.uniform(size=50)\n",
    "y_original = x*x + 10*np.sin (3*x)\n",
    "y_observed = y_original+12*np.random.normal(size=len(x))\n",
    "y_test = x*x + 10*np.sin (3*x) + 12*np.random.normal(size=len(x))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.scatter(x,y_original)\n",
    "plt.subplot(122)\n",
    "plt.scatter(x,y_observed)\n",
    "plt.gcf().set_size_inches((12,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want are forced to use a linear classifier to solve this problem. How can we model the non-linear behavior of this data set? We can, for example, add powers of the input features, i.e. $\\{x,x^2,x^3,x^4\\}$, and solve the linear regression problem. Let us show this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extended = np.c_[x,x*x,x*x*x,x*x*x*x]\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(x.reshape(-1,1),y_observed)\n",
    "xp = np.linspace(0,10,100)\n",
    "yhat = clf.predict(xp.reshape(-1,1))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y_observed)\n",
    "plt.plot(np.linspace(0,10,100),yhat)\n",
    "\n",
    "\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(x_extended,y_observed)\n",
    "xp = np.linspace(0,10,100)\n",
    "xp = np.c_[xp,xp*xp,xp*xp*xp,xp*xp*xp*xp]\n",
    "yhat = clf.predict(xp)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y_observed)\n",
    "plt.plot(np.linspace(0,10,100),yhat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling churn means to understand what keeps the customer engaged to our product. Its analysis goal is to predict or describe the **churn rate** i.e. the rate at which customer leave or cease the subscription to a service. Its value lies in the fact that engaging new customers is often more costly than retaining existing ones. For that reason subscription business-based companies usually have proactive policies towards customer retention.\n",
    "\n",
    "In this case study, we aim at building a machine learning based model for customer churn prediction on data from a Telecom company. Each row on the dataset represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer.\n",
    "\n",
    "This case is partially inspired in Eric Chiang's analysis of churn rate. Data is available from the University of California Irvine machine learning repositories data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The complete set of attributes is the following:\n",
    "\n",
    "+ State: categorical, for the 50 states and the District of Columbia\n",
    "+ Account length: integer-valued, how long an account has been active \n",
    "+ Area code: categorical\n",
    "+ Phone number: customer ID\n",
    "+ International Plan: binary feature, yes or no\n",
    "+ VoiceMail Plan: binary feature, yes or no\n",
    "+ Number of voice mail messages: integer-valued\n",
    "+ Total day minutes: continuous, minutes customer used service during the day\n",
    "+ Total day calls: integer-valued\n",
    "+ Total day charge: continuous\n",
    "+ Total evening minutes: continuous, minutes customer used service during the evening\n",
    "+ Total evening calls: integer-valued\n",
    "+ Total evening charge: continuous\n",
    "+ Total night minutes: continuous, minutes customer used service during the night\n",
    "+ Total night calls: integer-valued\n",
    "+ Total night charge: continuous\n",
    "+ Total international minutes: continuous, minutes customer used service to make international calls\n",
    "+ Total international calls: integer-valued\n",
    "+ Total international charge: continuous\n",
    "+ Number of calls to customer service: integer-valued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "churn_df = pd.read_csv('./files/churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "print (\"Column names:\")\n",
    "print (col_names)\n",
    "\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "\n",
    "print (\"\\nSample data:\")\n",
    "churn_df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px\">\n",
    "**QUESTION:** What shoulg we do with this dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting some baseline value by removing all **weird** variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=churn_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (col_names)\n",
    "\n",
    "to_drop = np.array(['State','Phone',\"Int'l Plan\",\"VMail Plan\",'Churn?'])\n",
    "\n",
    "idx_to_drop = np.array([np.where(np.array(col_names) == tmp)[0] for tmp in to_drop])\n",
    "\n",
    "fancy_index =  np.setdiff1d(np.arange(20),idx_to_drop)\n",
    "\n",
    "data_removed = data[:,fancy_index]\n",
    "\n",
    "print (data_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n",
    "\n",
    "# We don't need these columns\n",
    "to_drop = ['State','Phone',\"Int'l Plan\",\"VMail Plan\",'Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1)\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "\n",
    "X_removed = churn_feat_space.as_matrix().astype(np.float)\n",
    "\n",
    "print (\"Feature space holds %d observations and %d features\" % X_removed.shape)\n",
    "print (\"Unique target labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_removed,y,train_size=0.5,random_state=42)\n",
    "\n",
    "knn.fit(X_train,y_train) \n",
    "print(knn.score(X_train,y_train))\n",
    "print(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_removed,y,train_size=0.5,random_state=42)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "knn.fit(X_scaled,y_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print (knn.score(X_scaled,y_train))\n",
    "print (knn.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dealing with categorical data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding aka Dummy variables\n",
    "\n",
    "Dummy variables recode one feature into $K-1$ new features, where $K$ is the amount of values the original feature has. So for exemple, feature ``color`` has three values {red,blue,green}. We can replace this by $K-1 = 2$ new features, i.e. ``is_green?`` and ``is_blue?``. These new features have now two values, either $1$ if the question is true, or $0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "print ('There are ' +str(np.unique(churn_df[\"Int'l Plan\"]).shape[0]) +' unique values for this feature.')\n",
    "dummies = pd.get_dummies(churn_df[\"Int'l Plan\"])\n",
    "dummies_py = [1. if x=='yes' else  0. for x in churn_df[\"Int'l Plan\"]]\n",
    "dummies_numpy = np.where(churn_df[\"Int'l Plan\"]=='yes',1.,0.)\n",
    "\n",
    "print(len(dummies_py))\n",
    "print(dummies_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! well pandas literally takes the word 'dummy' !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this feature back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_df['IntYes'] = dummies_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and drop the international plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.drop([\"Int'l Plan\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class =  \"alert alert-success\" style= \"border-radius:10px\"> **EXERCISE**\n",
    "Replace adequate variables with the corresponding dummies. This is `state`, `Int'l Plan`, and `VMail Plan`. Replace also `Churn?` if you have not alredy done that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_dummies,y,train_size=0.5,random_state=42)\n",
    "\n",
    "knn.fit(X_train,y_train) \n",
    "print (knn.score(X_train,y_train))\n",
    "print (knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_dummies,y,train_size=0.5,random_state=42)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "knn.fit(X_scaled,y_train) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print (knn.score(X_scaled,y_train))\n",
    "print (knn.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_test_scaled[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Numerical data preprocessing - normalization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that numerical data in different ranges hinder the distance computation. Standarization solves this problem. Let us recap what standarization is about.\n",
    "\n",
    "Standarization corresponds to apply the following transformation to each coordinate:\n",
    "\n",
    "$$ \\tilde{x} = \\frac{x-\\mu_x}{\\sigma_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us code it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def myStandardScaler(X, mux=None, stx=None):\n",
    "    if mux is None:\n",
    "        mux = np.mean(X,axis = 0)\n",
    "    if stx is None:\n",
    "        stx = np.std(X,axis = 0)\n",
    "    return (X - np.tile(mux,(X.shape[0],1)))/np.tile(stx,(X.shape[0],1)), mux, stx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it now to the problem at hand considering the proper rules for training testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_dummies,y,train_size=0.5,random_state=42)\n",
    "\n",
    "X_scaled,mux,stx = myStandardScaler(X_train)\n",
    "knn.fit(X_scaled,y_train) \n",
    "X_test_scaled = myStandardScaler(X_test,mux=mux,stx = stx)[0]\n",
    "\n",
    "print (knn.score(X_scaled,y_train))\n",
    "print (knn.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of comparing variables in different scale ranges is the following normalization:\n",
    "    \n",
    "$$\\tilde{x} = \\frac{x-\\min(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "Let us try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myMinMaxScaler(X, mi=None, mx=None):\n",
    "    if mi is None:\n",
    "        mi = np.min(X,axis = 0)\n",
    "    if mx is None:\n",
    "        mx = np.max(X,axis = 0)\n",
    "\n",
    "    return (X - np.tile(mi,(X.shape[0],1)))/np.tile(mx-mi,(X.shape[0],1)), mi, mx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_dummies,y,train_size=0.5,random_state=42)\n",
    "\n",
    "X_scaled,mi,mx = myMinMaxScaler(X_train)\n",
    "knn.fit(X_scaled,y_train) \n",
    "X_test_scaled = myMinMaxScaler(X_test,mi=mi ,mx = mx)[0]\n",
    "\n",
    "print (knn.score(X_scaled,y_train))\n",
    "print (knn.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe though that all normalization techniques are heavily influenced by **outliers**. It is a good practice to detect outliers and potentially remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to really apply this technique we have to remove **outliers**. These are values that are far away from the rest. Outliers can be due to the inner dynamics of the measurement process (for example if it belongs to a heavy-tail distribution) or can be just an error. Let me just start saying that the notion of outliers is highly subjective. In words of Hawkins, 1980:\n",
    "\n",
    ">\"An outlier is an observation which deviates so much from the other\n",
    "observations as to arouse suspicions that it was generated by a different\n",
    "mechanism”\n",
    "\n",
    "\n",
    "When we look for outliers we could consider different approaches:\n",
    "\n",
    "+ Statistical description approaches: Consider that data is generated by some distribution. Infer the parameters of the distribution and score points accordingly. Outliers are those points with very small probability of belonging to the model.\n",
    "+ Geometric considerations: Outliers are those points located at the external boundary of the data set. This can be modelled using the notion of *convex hulls*.\n",
    "+ Distance based approaches: We can use the distance to the neighbors (we can use k-NN) to check if a point is an outlier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple approach to outlier removal and normalization is to consider a percentile, $prc$, of the data as outliers and normalize according to those values. \n",
    "\n",
    "$$\\tilde{x}=\\frac{x-prc(x,\\theta)}{prc(x,100-\\theta)-prc(x,\\theta)}$$\n",
    "\n",
    "where $\\theta$ is the percentile we consider as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myPrcScaler(X, mi=None, mx=None, theta=1.):\n",
    "    if mi is None:\n",
    "        mi = np.percentile(X,theta,axis=0)\n",
    "    if mx is None:\n",
    "        mx = np.percentile(X,100-theta,axis=0)\n",
    "\n",
    "    return (X - np.tile(mi,(X.shape[0],1)))/(np.tile(mx-mi,(X.shape[0],1))+1e-16), mi, mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_dummies,y,train_size=0.5,random_state=42)\n",
    "\n",
    "X_scaled,mi,mx = myPrcScaler(X_train, theta=2.)\n",
    "knn.fit(X_scaled,y_train) \n",
    "X_test_scaled = myPrcScaler(X_test,mi=mi ,mx = mx)[0]\n",
    "\n",
    "print (knn.score(X_scaled,y_train))\n",
    "print (knn.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Hashing\n",
    "\n",
    "Feature Hashing is like the encoding we saw before. However we use a hashing function that given the input category returns the value of the index where the one is located. This trick allows to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we are interested in searching for a value $a$ in a long list. \n",
    "\n",
    "+ If it is an ordered list, there are really fast algorithms ($O(\\mbox{log }n)$) to implement the search.\n",
    "+ If it is not an ordered list, looking for a value needs the full reading of the list ($O(n)$).\n",
    "\n",
    "Is it possible to do it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we have a list $L$ with an associated *magic function* that answer this kind of questions: \n",
    "\n",
    "> If $a$ would be stored in the list, in which position would be located?\n",
    "\n",
    "The function will always return an index $i$. Then, if $a$ is present in $L[i]$, we have solved the problem. If not, we conclude that $a$ was not in the list.\n",
    "\n",
    "**This kind of functions exists and they are called $hash$ functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to store a set of elements $\\{ a_i \\}$ in a list with this magic function, this function must have the following properties:\n",
    "\n",
    "+ It must be **random**: it randomly distributes data among all possible indexes.\n",
    "+ It must be **consistent**: the assignation rule must be constant and well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing a hash function for a problem, we must take into account the data distribution.\n",
    "\n",
    "If incoming data is represented by integer values with uniform distribution, we can use a random assignment rule, where the input parameter is data and the output parameter an index) and that's all!\n",
    "\n",
    "+ If our data is $ a \\in [0,N-1]$, and we have a list $L$ with N positions, the hash value can be $a$. If the list has n < N positions, the hash value can be $a \\% n$.\n",
    "\n",
    "If data is not uniformly distributed, we must design a function that generates uniformly distributed indexes from data values.\n",
    "\n",
    "*Note*: When two different samples are assigned to the same position we have a **collision**. We can deal with this problem in easy ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash functions.\n",
    "\n",
    "When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. \n",
    "\n",
    "For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.\n",
    "\n",
    "In general, the scheme for hashing such data is to break the input into a sequence of small units (bits, bytes, words, etc.) and combine all the units $b[1], b[2], ..., b[m]$ sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StringHash(a, m=257, C=1024):\n",
    "    \n",
    "# m represents the estimated cardinality of the items set\n",
    "# C represents a number that is larger that ord(c)\n",
    "    hash=0\n",
    "    for i in range(len(a)):\n",
    "        hash = (hash * C + ord(a[i])) % m\n",
    "    return hash\n",
    "\n",
    "print (StringHash('hola'), StringHash('adios'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more complex hashing techniques such as MurMur, SHA, etc.\n",
    "\n",
    "The hashing trick in the end avoids the fact of having to build the complete set of features since given the raw input returns the column index where the one is located. The hashing trick can work as the former technique or we may parse/process more complex data similar to bag of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the intuition of what goes on when one applies the Hashing Trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.random.normal(size=300)\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.hist(dat)\n",
    "plt.subplot(122)\n",
    "ndat=[StringHash(str(item)) for item in dat]\n",
    "plt.hist(ndat)\n",
    "plt.gcf().set_size_inches((12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check some value distances in both the original domain and the hashed domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0000000\n",
    "b = 1.0000001\n",
    "c = 109000.00\n",
    "\n",
    "print ('Value of \"a\" : '+ str(a))\n",
    "print ('Value of \"b\" : '+ str(b))\n",
    "print ('Value of \"c\" : '+ str(c))\n",
    "print ('Distance between a and b in the original space: ' + str(np.sqrt((b-a)*(b-a))))\n",
    "print ('Distance between a and c in the original space: ' + str(np.sqrt((c-a)*(c-a))))\n",
    "\n",
    "ha = StringHash(str(a))\n",
    "hb = StringHash(str(b))\n",
    "hc = StringHash(str(c))\n",
    "\n",
    "print ('Distance between a and b in the hashed space: ' + str(np.sqrt((hb-ha)*(hb-ha))))\n",
    "print ('Distance between a and c in the hashed space: ' + str(np.sqrt((hc-ha)*(hc-ha))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing trick intuitive idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text example: \n",
    "We can use a similar idea to the bag of words, i.e. to count the number of times each word appears in a text. In the bag of words scenario we have to define the set of words composing the dictionary beforehand. This implies checking all available words or define a very large feature space. By using the `hashing trick` we can define the dimensionality of the embedding beforehand without actually define the dictionary. This will come at the price of finding collisions.\n",
    "\n",
    "Consider that we want to embed the following text using the hashing function $h$ with 6 bins (The representation vector is of dimension 6):\n",
    "\n",
    "`My friend lives nearby.`\n",
    "\n",
    "A sensible way of doing so is defining a partition of this text. The obvious partition is word-based. Thus we are going to compute the hashing function of each of the words composing the sentence. This is h(My), h(friend), h('lives'), h('nearby.').\n",
    "\n",
    "Let us do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'My friend lives nearby.'\n",
    "\n",
    "bins = [StringHash(item,m=6, C=8) for item in sentence.split(' ')]\n",
    "print ('Hash values: ' + str(bins))\n",
    "\n",
    "representation = np.zeros((6,1))\n",
    "for idx in bins:\n",
    "    representation[idx]+=1\n",
    "    \n",
    "print ('Representation vector: \\n' + str(representation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADVANTAGES**\n",
    "+ The vectors will usually be very sparse. We can store them efficiently.\n",
    "+ We can increase the complexity of the hashing function to prevent collisions. For example, using **bloom filters**. Or we could use another hashing function with two values +,- that encodes if we have to add 1 to that position or substract 1.\n",
    "+ We don't need to prepare dictionaries or structures. This makes this approach real time and online friendly.\n",
    "+ Distribution of the hashed data tends to the uniform distribution.\n",
    "\n",
    "**DISADVANTAGES**\n",
    "+ Metric notion in the original space disappears in the hashed space. There are metric sensitive hashing techniques such as **Locality Sensitive Hashing**.\n",
    "+ One has to set in advance the dimensionality of the embedding space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">\n",
    "<b>EXERCISE:</b> \n",
    "\n",
    "We want to check if the variable `State` from the `churn.csv` data set conveys discriminant information. In order to do so we will encode this variable using one-hot encoding and feature hashing and check the prediction accuracy of on the `Churn?` target.\n",
    "<p>\n",
    "\n",
    "<b>TO DO:</b>\n",
    "<ol>\n",
    "<li>Create a data set with the one-hot encoding of the `State` variable.</li>\n",
    "<li>Create a data set with the hashing encoding of the `State` variable.</li>\n",
    "<li>Use a 3-Nearest neighbor classifier and report the average accuracy in both datasets in a test set of size 30% of the full data set; use `random_state=42`.</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style = \"border-radius:10px\">**QUIZ:** What of the following sentences is true,\n",
    "\n",
    "<ol>\n",
    "  <li> The feature is useful for the classifier using one-hot encoding.</li>\n",
    "  <li> The feature is useful for the classifier using the hashing trick.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Curse of dimensionality\n",
    "\n",
    "<small>Based on the discussion of Pedro Domingo's \"A Few Useful Things to Know About Machine Learning\".</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, does this means that we can add features at will? In principle yes, as long as they are informative. Let us check what happen when we add random data to our data set.\n",
    "\n",
    "\n",
    ">The expression of *Curse of dimensionality* was coined by Bellman in 1961 to refer to the fact that many algo- rithms that work fine in low dimensions become intractable when the input is high-dimensional. But in machine learning it refers that generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grows, because a fixed-size training set covers a dwindling fraction of the input space. -- Pedro Domingo's \"A Few Useful Things to Know About Machine Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Nearest neighbor in front of increasing dimensions\n",
    "\n",
    "Similarity-based reasoning that machine learning algorithms depend on (explicitly or implicitly) breaks down in high dimensions. Consider a nearest neighbor classifier with Hamming distance as the similarity measure, and suppose the class is just x1 ∧ x2. If there are no other features, this is an easy problem. But if there are 98 irrelevant features x3,...,x100, the noise from them completely swamps the signal in x1 and x2, and nearest neighbor effectively makes random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X=data.data/16.\n",
    "y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_dims = 1000\n",
    "X_fake = np.random.randn(X.shape[0],fake_dims)\n",
    "\n",
    "X_new = np.c_[X,X_fake]\n",
    "\n",
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(X_new,y,train_size=0.5,random_state=42)\n",
    "\n",
    "knn.fit(X_train,y_train) \n",
    "print ('Train accuracy ' + str(knn.score(X_train,y_train)) + '// Test accuracy ' + str(knn.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The grid effect and the number of neighbors\n",
    "\n",
    "Even more disturbing is that nearest neighbor still has a problem even if all 100 features are relevant! This is because in high dimensions all examples look alike. Suppose, for instance, that examples are laid out on a regular grid, and consider a test example xt. If the grid is d-dimensional, xt’s 2d nearest examples are all at the same distance from it. So as the dimensionality increases, more and more examples become nearest neighbors of xt, until the choice of nearest neighbor (and therefore of class) is effectively random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Our intuition on Gaussian distributions and the hyper-orange peel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only one instance of a more general problem with high dimensions: our intuitions, which come from a three- dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hyper-sphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hyper- sphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**QUESTION:** Consider a $d$ dimensional Gaussian distribution with zero mean and identity covariance matrix. Do you think that drawing a sample at random will be close to the mean with high probability?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "d=100\n",
    "N=1000000\n",
    "X=np.random.multivariate_normal(np.zeros(d,),np.eye(d,),N)\n",
    "d = np.linalg.norm(X,axis=1)\n",
    "plt.hist(d,bins=30)\n",
    "plt.xlabel('Distance to the mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations on large dimensional spaces.** CHECK MACKAY; MURPHY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Benefits of the curse of dimensionality\n",
    "\n",
    "Building a classifier in two or three dimensions is easy; we\n",
    "Test-Set Accuracy (%)\n",
    "can find a reasonable frontier between examples of different classes just by visual inspection. (It’s even been said that if people could see in high dimensions machine learning would not be necessary.) But in high dimensions it’s hard to understand what is happening. This in turn makes it difficult to design a good classifier. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact their benefits may be outweighed by the curse of dimensionality.\n",
    "Fortunately, there is an effect that partly counteracts the curse, which might be called the “blessing of non-uniformity.” In most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near a lower-dimensional manifold. For example, k-nearest neighbor works quite well for handwritten digit recognition even though images of digits have one dimension per pixel, because the space of digit images is much smaller than the space of all possible images. Learners can implicitly take advantage of this lower effective dimension, or algorithms for explicitly reducing the dimensionality can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
